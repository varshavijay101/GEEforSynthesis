/**** Start of imports. If edited, may not auto-convert in the playground. ****/
var humanmod = ee.ImageCollection("CSP/HM/GlobalHumanModification"),
    Ecoregions2017 = ee.FeatureCollection("RESOLVE/ECOREGIONS/2017"),
    networks = ee.FeatureCollection("users/varshavijay101/networks");
/***** End of imports. If edited, may not auto-convert in the playground. *****/

////////// Case Study: Human modification and habitat type for//////////// 
//////////////////seed dispersal network samples ////////////////
 /*
 2020 Varsha Vijay GEEforSynthesis 
 https://sites.google.com/view/geeforsynthesis
 
*/

//This case study comes from a collaboration with Dr. Evan Fricke.
//Please read below for background

/*
Dr. Evan Fricke, an ecologist and fellow postdoctoral researcher at SESYNC, 
is trying to understand mutualistic plant-animal interactions 
through the study of seed dispersal networks. 
Using a new dataset composed of network observations around the world,
he wants to answer questions both about the broad ecological patterns of these networks
and the role that humans have in altering native interactions through
the introduction of native species.

Cool stuff!
*/

/*
To better characterize anthropogenic impacts in locations with
network observations, he wanted to use the Global Human Modification dataset.
Please search the bar above to learn more about this dataset.

I developed this code in GEE, because it takes advantage of 
some of the unique resources of this environment.
* Dataset is in the GEE data catalog
* GEE allows for to rapid calculation of statistics on image regions
*/

/*
Add Global Human Modification (gHM) dataset to code editor
print dataset
*/

/*
For our analysis we want to use the gHM data as an image
not an image collection. To do this we need to extract the single
image from the image collection.
This can be done by calling the image directly using the
Image id found when printing the collection like below
*/

//var name= ee.Image('imageid')

/*
or by selecting the first image in this image collection
or be filtering the image collection as shown in the previous collection.
Select the image using the first() function
*Note that we explicitly cast to an Image object type
when using these approaches*
*/

var ghm=ee.Image(humanmod.first())

/*
Comment out one of these approaches to get your final image and print
to verify object type and properties.
*/

/*
Great job!
Now visualize the gHM to understand the data better
using the visualization parameters provided below
Modify the visualiztion using the palette argument with 
either color names or hex colors
Use the appropriate Map. function
*/

//var vizparams= {min:0,max:1,palette: ['0000FF','FFFF00','FF0000']}//blue,yellow,red

/*
Because we are doing calculations for sample regions
we need to understand the spatial resolution of these data
Try this for the gHM data by modifying the code below
*/

//print(name.projection().nominalScale())

/*
This nominal scale is in agreement with the metadata
spatial resolution is nominal 1km
*/

/*
Now import the network sample locations from the Resources
page of the workshop website. The csv file is called
networks. *view README to see explanation and changes from original data
Import as csv and add to code editor.
Cast object to FeatureCollection.
Add to Map.
*/

var networks= ee.FeatureCollection(networks)

/*
If you do not see your point data then go back and Import, 
this time paying attention to the x,y fields of the import.
Make sure they match the Longitude and Latitude columns
of the original dataset exactly.
Now reimport with the correct values and visualize in Map.
/*

Map.addLayer(name,{},'name')//This syntax can be used to name but use standard viz

/*
See how many network sample locations are in the dataset
using the size() function and print to view data properties
*/

//print(name.size())

/*
Now that both the gHM data and network samples are loaded in the 
code editor we calculate the gHM index for all sites.

First we define a function to buffer the point sample site data 
so we can take the mean of several pixels instead of a single estimate
*/

var applybuff = function(feature) {
  return feature.buffer(5000);  //Define buffer in meters
};

/*
Now we need to apply a new concept, mapping across feature collections.
This applies the function to each feature in the feature collection
and takes the place of loops which are a no no in this environment.
In this simple case, the syntax is .map(functionname)
*/

var namebuff = networks.map(applybuff);
//Map.addLayer(name,{color: '00FF00'},'name')

/*
What is the mean gHM accross the buffered sample areas?
To answer this we use the reduceRegion function.
Reminder that this means that we are finding 
the mean image value across ALL regions. 
Later we will determine the mean value in EACH region.

Note the syntax...where does the Image go and where do the regions go?
Also note that I am explicitly setting the calculation using the
scale and projection of the input image.
For image calculations it can be good practice to set these explictly.

This returns a new type of object. print to see what this is.
*/

var nameghm= ghm.reduceRegion({
  geometry: namebuff,
  reducer: ee.Reducer.mean(),
  scale:ghm.projection().nominalScale()
  })
print(nameghm)

/*
The default calculation weights the calculation by the 
proportion of the pixel included in the image region.
See what this means by rerunning the same calculation with
the unweighted() function which assigns equal weight to all pixels
in the region
*/
var nameghmuw= ghm.reduceRegion({
  geometry: namebuff,
  reducer: ee.Reducer.mean().unweighted(),
  scale:ghm.projection().nominalScale()
  })

print(nameghmuw)

/*
In this case these values should be very close but this
is an important consideration for these types of analyses
on a more advanced level the splitWeights() function can be
used to assign explicit weights
*/

/*
While we answered one question, the more interesting questions
are about the relationship between individual sample estimates
of anthropogenic influence and gHM.

We run the reduceRegions function to answer this question.
This gives us the mean value for EACH sample location.
For interpretability I use the setOutputs function
assign the name 'meanghi' for the value.  
Default is 'mean'.
*/

var networkmodBUFF= ghm.reduceRegions({
  collection: namebuff,
  reducer: ee.Reducer.mean().setOutputs(['meanghi']),
  scale:ghm.projection().nominalScale()
  })

/*
YAY! Having determined mean gHM for each sample lets explore what
that means

Print the resulting dataset.  Note the object type! This is different
than the output of reduceRegion.
*/

/*
Finally, export the data as a csv to your Google drive.
Because the geometry column chews up unnecessary space we remove
that in the export code using the select function.
The last arguement to select is retainGeometry: "When false, the result will have a NULL geometry."
This is incredibly useful to speed up and reduce size of the
exported table
*/

Export.table.toDrive({
  collection: networkmodBUFF.select([".*"],null,false),
  description: 'networkmodbuff',
  folder: 'Evan',
  fileFormat:'CSV'
});

/*
Nothing happens!  That is because you need to click the Tasks
tab on the right and run the task.
Download from Google Drive to ensure the expected output.

At this stage we can finish and use these data for further
statistical analysis. Continue below for some more 
practice and to better understand the values we calculated
*/

/*
We calculated mean gHM values for all sample regions.  Let's explore these values
using the aggregate_stats function
*/

print('gHM stats:', name.aggregate_stats('meanghi'));

/*
Evan found greater numbers of invasive species and interactions
in islands. 

Is there a relationship between this and anthropogenic impacts (gHM)?
Using the ReduceRegion function determine whether oceanic islands 
have greater mean gHM than non-island samples.  How does this compare
to continental islands?
*/


/*
Use the filterMetadata function on the gHM dataset for network samples
(referenced above in the export and aggregate_stats call)
to identify oceanic islands, continental islands and non-island samples.
Review the dataset to see how this information is stored.
Add a group id to each sample in the group using a mapped function
Use the .size() function to determine how many samples fall into these categories
*/

var name=name.filterMetadata('property','equals','field')//use this or ee.Filter.eq shown below
.map(function(feature){return feature.set('group', 'string');});
//identify non-island samples using boolean AND
var nonisland=name
.filter(ee.Filter.and(
    ee.Filter.eq('property', field),
    ee.Filter.eq('property', field)))

/*
now rerun
*/



/*
Interesting! SE








